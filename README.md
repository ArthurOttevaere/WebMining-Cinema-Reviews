# ğŸ¬ Projet Web Mining : Analyse des Critiques de CinÃ©ma

**Cours :** MLSMM2153 - Web Mining (2025-2026)  
**Professeurs :** Corentin Vande Kerckhove & Sylvain Courtain  
**Sujet 4 :** Analyse des critiques culturelles sur des blogs (CinÃ©ma)

---

## ğŸ‘¥ L'Ã‰quipe

* **Arthur Ottevaere**
* **Mohamed Amine El Mohcine**
* **Lenny Andry**

---

## ğŸ“– Contexte et Objectifs

Ce projet a pour but d'analyser les critiques cinÃ©matographiques se trouvant sur des blogs en ligne. Dans ce projet, nous collectons et analysons un total de 900 critiques provenant d'un des blogs cinÃ©matographiques anglophones de rÃ©fÃ©rence : `https://www.rogerebert.com`. L'objectif est d'y dÃ©celer des tendances sÃ©mantiques et structurelles.

Le projet suit une dÃ©marche classique de web mining, Ã  savoir :

1. **Collecte de donnÃ©es (Scraping) :** RÃ©cupÃ©ration automatique de corpus massifs (textes, notes, mÃ©tadonnÃ©es, casting).
2. **Text Mining :** PrÃ©traitement linguistique (NLP/Lemmatisation), vectorisation (TF-IDF), analyse lexicographique et sÃ©mantique, et identification de thÃ©matiques latentes (Clustering K-Means).
3. **Link Analysis :** ModÃ©lisation d'un graphe sÃ©mantique non orientÃ©, analyse de la topologie rÃ©seau (dÃ©tection d'Ã®lots, Small World) et identification des Å“uvres influentes via des mesures de centralitÃ© et de prestige (PageRank, Information Centrality).

---

## ğŸ“‚ Structure du Projet

L'architecture respecte la sÃ©paration entre code source, donnÃ©es brutes et rÃ©sultats dans le but de faciliter la rÃ©plication des analyses.

```text
.
â”œâ”€â”€ main.py                 # Fonction de lancement du projet (pipeline)
â”œâ”€â”€ src/                    # Code source Python
â”‚   â”œâ”€â”€ scraping            # Scripts de collecte des donnÃ©es (RogerEbert)
â”‚   â”œâ”€â”€ text_mining         # Scripts de transformation et d'analyse du contenu textuel des critiques
â”‚   â”œâ”€â”€ link_analysis       # Scripts de construction du graphe et d'analyse des liens
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # DonnÃ©es brutes issues du scraping, text-mining et link-analysis (.csv/.xlsx)
â”‚   â”‚                       # Note : Ces fichiers ne sont pas versionnÃ©s sur GitHub (via .gitignore)
â”‚   â””â”€â”€ processed/          # DonnÃ©es nettoyÃ©es prÃªtes pour l'analyse
â”‚
â”œâ”€â”€ results/                # Graphiques et visualisations
â”œâ”€â”€ .gitignore              # Configuration des fichiers exclus (env, donnÃ©es lourdes)
â”œâ”€â”€ requirements.txt        # Liste des dÃ©pendances Python nÃ©cessaires
â””â”€â”€ README.md               # Documentation du projet
```

---

## ğŸš€ Guide d'Utilisation (Pipeline)

### 1. Installation

Assurez-vous d'avoir Python 3.9+ installÃ©. Clonez le repo et installez les dÃ©pendances (`requirements.txt`) :

```Bash
git clone https://github.com/ArthurOttevaere/WebMining-Cinema-Reviews.git
cd WebMining-Cinema-Reviews
pip install -r requirements.txt
```

### 2. ExÃ©cution des analyses

L'ensemble du pipeline (Scraping, Text mining et Link analysis) est orchestrÃ© par un script unique (`main.py`) afin d'assurer une meilleure rÃ©plicabilitÃ©. Alors, pour lancer l'analyse complÃ¨te du projet, il suffit d'entrer la commande suivante dans votre terminal :

```Bash
python main.py
```

Ce script exÃ©cute, en arriÃ¨re-plan, les Ã©tapes suivantes :

* **Chargement des donnÃ©es :** Par dÃ©faut, le script charge le dataset fourni `data/processed/reviews_final_900.csv` pour Ã©viter une nouvelle collecte longue des donnÃ©es. Cela permet Ã©galement d'obtenir les mÃªmes rÃ©sultats que ceux illustrÃ©s dans le rapport et dans l'ensemble de l'analyse.

* **Text mining :** Nettoyage, vectorisation TF-IDF et clustering des critiques cinÃ©matographiques. Des visuels relatifs Ã  l'analyse descriptive et sÃ©mantique apparaitront au lancement du code.

* **Construction du graphe :** GÃ©nÃ¨re des noeuds et des arÃªtes sur base de la similaritÃ© cosinus. Ces "Nodes" et "Edges" sont directement calculÃ©es via le corpus de donnÃ©es scrapÃ© (`data/processed/reviews_final_900.csv`).

* **Link analysis :** Analyse structurelle via calcul matriciel. Le script gÃ©nÃ¨re les mÃ©triques de centralitÃ© et de prestige clÃ©s (*PageRank*, *Information Centrality*, *Closeness*, *Betweenness*), analyse la topologie globale (DiamÃ¨tre, Rayon) et visualise les distances moyennes entre les thÃ¨mes via une *Heatmap*.

### **âš ï¸ Note importante concernant le Scraping (`RUN_SCRAPER = False`)**

Par dÃ©faut, la collecte de nouvelles donnÃ©es est dÃ©sactivÃ©e pour garantir la **stricte rÃ©plicabilitÃ© des rÃ©sultats** prÃ©sentÃ©s dans notre rapport.

Bien que le module de scraping soit complet et fonctionnel (importÃ© via `src.scraping`), nous vous recommandons vivement de **ne pas passer cette variable Ã  `True`**, car :

1. **CohÃ©rence :** Le site *RogerEbert.com* Ã©tant dynamique, une nouvelle collecte modifierait le corpus. Les clusters et mÃ©triques de graphe divergeraient alors de ceux analysÃ©s dans le PDF rendu.

2. **Performance :** L'analyse s'exÃ©cute ici instantanÃ©ment sur le jeu de donnÃ©es figÃ© (`data/processed/reviews_final_900.csv`), alors qu'un nouveau scraping prendrait un temps plus consÃ©quent.

Le code de scraping est inclus dans le projet Ã  des fins de dÃ©monstration mÃ©thodologique et de vÃ©rification technique uniquement.

---

## ğŸ§  MÃ©thodologie et Concepts ClÃ©s

### Scraping

La constitution du corpus repose sur une stratÃ©gie de navigation *Breadth-First Search (BFS)* ciblÃ©e sur le site `https://www.rogerebert.com`.

* **Approche :** Utilisation d'un systÃ¨me de file d'attente (Queue) initialisÃ© par des critiques rÃ©centes (Seeds). Le script ne collecte pas au hasard mais suit les citations entre critiques pour garantir une cohÃ©rence sÃ©mantique. Une contrainte de profondeur (depth < 2) limite l'exploration aux voisins immÃ©diats et secondaires, garantissant un corpus centrÃ© sur les citations directes sans divergence exponentielle.

* **Outils :** `BeautifulSoup` pour le parsing HTML et extraction structurÃ©e (Titre, Score, MÃ©tadonnÃ©es, Texte).

* **Volume :** Corpus final de 900 critiques structurÃ©es.

### Text Mining

Le pipeline de traitement du langage naturel vise Ã  transformer le texte brut en indicateurs quantitatifs et sÃ©mantiques.

* **PrÃ©traitement AvancÃ© :** Nettoyage *Regex* suivi d'un *POS-Tagging* (via NLTK) pour identifier et exclure les entitÃ©s nommÃ©es (Noms propres) et lemmatiser conditionnellement les verbes.

* **Vectorisation :** ModÃ¨le TF-IDF (Unigrams & Bigrams) avec filtrage frÃ©quentiel (`min_df=2, max_df=0.5`).

* **RÃ©duction de Dimension :** Application d'une SVD (Singular Value Decomposition) Ã  150 composantes suivie d'une normalisation L2.

* **Analyse de Sentiment :** Utilisation de *VADER* pour l'analyse de polaritÃ© et la segmentation des trajectoires narratives.

* **Clustering :** Algorithme K-Means (K=12, validÃ© par score Silhouette) pour identifier les thÃ©matiques latentes (ex: Horreur, Musical, Guerre).

### Link Analysis (Approche Matricielle)

La modÃ©lisation du rÃ©seau dÃ©passe l'utilisation de librairies "boÃ®te noire". Nous avons implÃ©mentÃ© les algorithmes via Numpy et l'algÃ¨bre linÃ©aire pure.

#### ğŸš§ Construction du Graphe

StratÃ©gie hybride "Cluster-First" basÃ©e sur la similaritÃ© cosinus :

* **Liens Intra-Cluster :** Densification locale (4 voisins, seuil > 0.30).

* **Liens Inter-Cluster :** Ponts sÃ©mantiques (1 voisin, seuil strict > 0.50).

* **Filtre SÃ©mantique :** Application d'une Custom Stop-list (termes gÃ©nÃ©riques du cinÃ©ma) pour forcer des connexions basÃ©es sur le fond thÃ©matique.

#### ğŸ“Š MÃ©triques & Algorithmes ImplÃ©mentÃ©s

* **PageRank :** CalculÃ© via la mÃ©thode des puissances (Power Iteration) sur le graphe non orientÃ©. Forte corrÃ©lation observÃ©e avec le DegrÃ© (0.93).

* **Information Centrality :** Utilisation de la Pseudo-Inverse du Laplacien pour identifier les "nÅ“uds ponts" (films charniÃ¨res).

* **Betweenness Centrality :** Utilise l'algorithme de Freeman afin de dÃ©celer les noeuds qui agissent comme des goulots d'Ã©tranglements.

* **Topologie (Floyd-Warshall) :** Calcul de la matrice des plus courts chemins pour dÃ©river :

    *Closeness Centrality & ExcentricitÃ©.
    *DiamÃ¨tre (15) et Rayon (1), rÃ©vÃ©lant la prÃ©sence d'Ã®lots dÃ©connectÃ©s.
    *Heatmap inter-clusters : Visualisation des distances moyennes (sauts) entre les thÃ¨mes.

* **Partitionnement Spectral :** Calcul du Vecteur de Fiedler (valeurs propres du Laplacien) pour couper le graphe en deux communautÃ©s structurelles Ã©quilibrÃ©es.

---

## ğŸ“Š AperÃ§u des RÃ©sultats

### Visualisation Gephi

![Graphe Gephi](results/link_analysis_graph.png)
*LÃ©gende : Les couleurs reprÃ©sentent les thÃ¨mes (Clusters) identifiÃ©s par TF-IDF dans la partie de l'analyse de liens.*

### Distances Inter-Clusters (Topologie)

![Matrice Heatmap](results/link_analysis_matrix.png)
*LÃ©gende : Matrice de distance moyenne (sauts) entre les thÃ¨mes. On observe une proximitÃ© structurelle entre la plupart des clusters, tandis que le cluster "Musical" apparaÃ®t plus isolÃ©.*
