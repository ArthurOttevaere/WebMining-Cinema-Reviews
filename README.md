# üé¨ Projet Web Mining : Analyse des Critiques de Cin√©ma

**Cours :** MLSMM2153 - Web Mining (2025-2026)  
**Professeurs :** Corentin Vande Kerckhove & Sylvain Courtain  
**Sujet 4 :** Analyse des critiques culturelles sur des blogs (Cin√©ma)

---

## üë• L'√âquipe

* **Arthur Ottevaere**
* **Mohamed Amine El Mohicine**
* **Lenny Andry**

---

## üìñ Contexte et Objectifs

Ce projet a pour but d'analyser les critiques cin√©matographiques se trouvant sur des blogs en ligne. Dans ce projet, nous collectons et analysons un total de 900 critiques provenant d'un des blogs cin√©matographiques anglophones de r√©f√©rence : `https://www.rogerebert.com`. L'objectif est d'y d√©celer des tendances s√©mantiques et structurelles. 

Le projet suit le m√™me cheminement que le cours de Web Mining, √† savoir :

1. **Collecte de donn√©es (Scraping) :** R√©cup√©ration automatique de corpus massifs (textes, notes, m√©tadonn√©es, casting).
2. **Text Mining :** Pr√©traitement linguistique (NLP/Lemmatisation), analyse de sentiments (VADER), vectorisation (TF-IDF) et identification de th√©matiques latentes (Clustering K-Means).
3. **Link Analysis :** Mod√©lisation d'un graphe s√©mantique non orient√©, analyse de la topologie r√©seau (d√©tection d'√Ælots, Small World) et identification des ≈ìuvres influentes via mesures de centralit√© (PageRank, Information Centrality).

---

## üìÇ Structure du Projet

L'architecture respecte la s√©paration entre code source, donn√©es brutes et r√©sultats. Dans le but de faciliter la r√©plication des analyses.

```text
.
‚îú‚îÄ‚îÄ main.py                 # Fonction de lancement du projet (pipeline)
‚îú‚îÄ‚îÄ src/                    # Code source Python
‚îÇ   ‚îú‚îÄ‚îÄ scraping            # Scripts de collecte des donn√©es (RogerEbert)
‚îÇ   ‚îú‚îÄ‚îÄ text_mining         # Scripts de transformation et d'analyse du contenu textuel des critiques
‚îÇ   ‚îú‚îÄ‚îÄ link_analysis       # Scripts de construction du graph et d'analyses des liens
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                # Donn√©es brutes issues du scraping, text-mining et link-analysis (.csv/.xlsx)
‚îÇ   ‚îÇ                       # Note : Ces fichiers ne sont pas versionn√©s sur GitHub (via .gitignore)
‚îÇ   ‚îî‚îÄ‚îÄ processed/          # Donn√©es nettoy√©es pr√™tes pour l'analyse
‚îÇ
‚îú‚îÄ‚îÄ results/                # Graphiques, visualisations et rapports (A SUPPRIMER)
‚îú‚îÄ‚îÄ .gitignore              # Configuration des fichiers exclus (env, donn√©es lourdes)
‚îú‚îÄ‚îÄ requirements.txt        # Liste des d√©pendances Python n√©cessaires
‚îî‚îÄ‚îÄ README.md               # Documentation du projet
```

---

## üöÄ Guide d'Utilisation (Pipeline)

### 1. Installation

Assurez-vous d'avoir Python 3.9+ install√©. Clonez le repo et installez les d√©pendances :

```Bash
git clone https://github.com/ArthurOttevaere/WebMining-Cinema-Reviews.git
cd WebMining-Cinema-Reviews
pip install -r requirements.txt
```

### 2. Ex√©cution des analyses

L'ensemble du pipeline (Scraping, Text mining et Link analysis) est orchestr√©e par un script unique (`main.py`) afin d'assurer une meilleure r√©plicabilit√©. Alors, pour lancer l'analyse compl√®te du projet, il suffit d'entrer la commande suivante dans votre terminal :

```Bash
python main.py
```

Ce script ex√©cute, en arri√®re-plan, les √©tapes suivantes :

* **Chargement des donn√©es :** Par d√©faut, le script charge le dataset fourni `data/processed/reviews_final_900.csv` pour √©viter une nouvelle collecte longue des donn√©es. Cela permet √©galement d'obtenir les m√™mes r√©sultats que ceux illustr√©s dans le rapport et dans l'ensemble de l'analyse.

* **Text mining :** Nettoyage, vectorisation TF-IDF et clustering des critiques cin√©matographiques. Des visuels relatifs √† l'analyse s√©mantique et de sentiment apparaitront au lancement du code.

* **Construction du graphe :** G√©n√®re des noeuds et des arr√™tes sur base de la similarit√© cosinus. Ces "Nodes" et "Edges" sont directement calcul√©es via le corpus de donn√©es scrap√© (`data/processed/reviews_final_900.csv`).

* **Link analysis :** Analyse structurelle via calcul matriciel. Le script g√©n√®re les m√©triques de centralit√© cl√©s (*PageRank*, *Information Centrality*, *Closeness*), analyse la topologie globale (Diam√®tre, Rayon) et visualise les distances moyennes entre les th√®mes via une *Heatmap*.

### **‚ö†Ô∏è Note importante concernant le Scraping (`RUN_SCRAPER = False`)**

Par d√©faut, la collecte de nouvelles donn√©es est d√©sactiv√©e pour garantir la **stricte r√©plicabilit√© des r√©sultats** pr√©sent√©s dans notre rapport.

Bien que le module de scraping soit complet et fonctionnel (import√© via `src.scraping`), nous vous recommandons vivement de **ne pas passer cette variable √† `True`**, car :

1. **Coh√©rence :** Le site *RogerEbert.com* √©tant dynamique, une nouvelle collecte modifierait le corpus. Les clusters et m√©triques de graphe divergeraient alors de ceux analys√©s dans le PDF rendu.

2. **Performance :** L'analyse s'ex√©cute ici instantan√©ment sur le jeu de donn√©es fig√© (`data/processed/reviews_final_900.csv`), alors qu'un nouveau scraping prendrait un temps plus cons√©quent.

Le code de scraping est inclus dans le projet √† des fins de d√©monstration m√©thodologique et de v√©rification technique uniquement.

---

## üß† M√©thodologie et Concepts Cl√©s

### Scraping

La constitution du corpus repose sur une strat√©gie de navigation *Breadth-First Search (BFS)* cibl√©e sur le site `https://www.rogerebert.com`.

* **Approche :** : Utilisation d'un syst√®me de file d'attente (Queue) initialis√© par des critiques r√©centes (Seeds). Le script ne collecte pas au hasard mais suit les citations entre critiques pour garantir une coh√©rence s√©mantique.

* **Outils :** `BeautifulSoup` pour le parsing HTML et extraction structur√©e (Titre, Score, M√©tadonn√©es, Texte).

* **Volume :** Corpus final de 900 critiques structur√©es.

### Text Mining

Le pipeline de traitement du langage naturel vise √† transformer le texte brut en indicateurs quantitatifs et s√©mantiques.

* **Pr√©traitement Avanc√© :** Nettoyage *Regex* suivi d'un *POS-Tagging* (via NLTK) pour identifier et exclure les entit√©s nomm√©es (Noms propres) et lemmatiser conditionnellement les verbes.

* **Vectorisation :** Mod√®le TF-IDF (Unigrams & Bigrams) avec filtrage fr√©quentiel (`min_df=2, max_df=0.5`).

* **R√©duction de Dimension :** Application d'une SVD (Singular Value Decomposition) √† 150 composantes suivie d'une normalisation L2.

* **Clustering :** Algorithme K-Means (K=12, valid√© par score Silhouette) pour identifier les th√©matiques latentes (ex: Horreur, Musical, Guerre).

* **Analyse de Sentiment :** Utilisation de *VADER* pour l'analyse de polarit√© et la segmentation des trajectoires narratives.

### Link Analysis (Approche Matricielle)

La mod√©lisation du r√©seau d√©passe l'utilisation de librairies "bo√Æte noire". Nous avons impl√©ment√© les algorithmes via Numpy et l'alg√®bre lin√©aire pure.

#### üöß Construction du Graphe

Strat√©gie hybride "Cluster-First" bas√©e sur la similarit√© cosinus :

* **Liens Intra-Cluster :** Densification locale (4 voisins, seuil > 0.30).

* **Liens Inter-Cluster :** Ponts s√©mantiques (1 voisin, seuil strict > 0.50).

* **Filtre S√©mantique :** Application d'une Custom Stop-list (termes g√©n√©riques du cin√©ma) pour forcer des connexions bas√©es sur le fond th√©matique.

#### üìä M√©triques & Algorithmes Impl√©ment√©s

* **PageRank :** Calcul√© via la m√©thode des puissances (Power Iteration) sur le graphe non orient√©. Forte corr√©lation observ√©e avec le Degr√© (0.93).

* **Information Centrality :** Utilisation de la Pseudo-Inverse du Laplacien pour identifier les "n≈ìuds ponts" (films charni√®res).

* **Topologie (Floyd-Warshall) :** Calcul de la matrice des plus courts chemins pour d√©river :

    * Closeness Centrality & Excentricit√©.
    * Diam√®tre (15) et Rayon (1), r√©v√©lant la pr√©sence d'√Ælots d√©connect√©s.
    * Heatmap inter-clusters : Visualisation des distances moyennes (sauts) entre les th√®mes.

* **Partitionnement Spectral :** Calcul du Vecteur de Fiedler (valeurs propres du Laplacien) pour couper le graphe en deux communaut√©s structurelles √©quilibr√©es.

---

## üìä Aper√ßu des R√©sultats

### Visualisation Gephi

![Graphe Gephi](results/link_analysis_graph.png)
*L√©gende : Les couleurs repr√©sentent les th√®mes (Clusters) identifi√©s par TF-IDF dans la partie de l'analyse de liens.*

### Distances Inter-Clusters (Topologie)

![Matrice Heatmap](results/link_analysis_matrix.png)
*L√©gende : Matrice de distance moyenne (sauts) entre les th√®mes. On observe une proximit√© structurelle entre la plupart des clusters, tandis que le cluster "Musical" appara√Æt isol√©.*

### Ajout potentiel d'un autre visuel pertinent

Peut-√™tre mettre le graphe avec Climax etc.