import pandas as pd
import os
import time

# --- IMPORTS DES SCRAPERS ---
# Assurez-vous que les fichiers de vos camarades sont bien dans le dossier src/
# et qu'ils ont bien une fonction principale qui retourne un DataFrame
from scraping_lwlies import launch_scraping_arthur
# from scraping_amine import lancer_scraping_amine  # D√©commenter quand pr√™t
# from scraping_lenny import lancer_scraping_lenny  # D√©commenter quand pr√™t

# Configuration
OUTPUT_RAW_PATH = 'data/raw/corpus_global_raw.csv'

def main():
    print("üé¨ D√âMARRAGE DE LA COLLECTE GLOBALE üé¨")
    
    # Liste pour stocker les DataFrames
    dfs_to_merge = []

    # --- 1. COLLECTE : LITTLE WHITE LIES (Arthur) ---
    try:
        print("\n‚ö° [1/3] Lancement du scraping Arthur...")
        # On peut limiter pour le test, ou mettre 300 pour la prod
        df_arthur = launch_scraping_arthur(limit=10) 
        
        if not df_arthur.empty:
            # IMPORTANT : On marque l'origine des donn√©es avant la fusion
            df_arthur['source_site'] = 'Little White Lies'
            dfs_to_merge.append(df_arthur)
            
    except Exception as e:
        print(f"‚ùå Erreur critique Arthur : {e}")

    # --- 2. COLLECTE : SITE AMINE ---
    # try:
    #     print("\n‚ö° [2/3] Lancement du scraping Amine...")
    #     df_amine = lancer_scraping_amine(limit=300)
    #     if not df_amine.empty:
    #         df_amine['source_site'] = 'Nom Site Amine' # √Ä adapter
    #         dfs_to_merge.append(df_amine)
    # except Exception as e:
    #     print(f"‚ùå Erreur critique Amine : {e}")

    # --- 3. COLLECTE : SITE LENNY ---
    # (M√™me logique...)

    # --- 4. FUSION ET SAUVEGARDE ---
    print("\n------------------------------------------------")
    
    if dfs_to_merge:
        print(f"üîÑ Fusion de {len(dfs_to_merge)} sources...")
        
        # C'est ici que la magie op√®re gr√¢ce √† vos colonnes identiques
        final_df = pd.concat(dfs_to_merge, ignore_index=True)
        
        # Petit nettoyage de s√©curit√© (doublons exacts)
        initial_len = len(final_df)
        final_df.drop_duplicates(subset=['article_url'], inplace=True)
        dedup_len = len(final_df)
        
        if initial_len != dedup_len:
            print(f"üßπ {initial_len - dedup_len} doublons supprim√©s.")

        # Sauvegarde
        final_df.to_csv(OUTPUT_RAW_PATH, index=False)
        final_df.to_excel(OUTPUT_RAW_PATH.replace('.csv', '.xlsx'), index=False)
        
        print(f"\n‚úÖ TERMIN√â ! Le corpus global est pr√™t.")
        print(f"üìä Total critiques : {len(final_df)}")
        print(f"üìÅ Fichier : {OUTPUT_RAW_PATH}")
        
        # Aper√ßu de la r√©partition
        print("\nR√©partition par source :")
        print(final_df['source_site'].value_counts())
        
    else:
        print("‚ùå Aucune donn√©e r√©cup√©r√©e. V√©rifiez les scrapers individuels.")

if __name__ == "__main__":
    main()